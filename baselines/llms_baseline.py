from transformers import AutoModelForCausalLM, AutoTokenizer
import json
import os
from tqdm import tqdm
import argparse

# å°è¯•ä½¿ç”¨vllmåŠ é€Ÿæ¨¡å‹æ¨ç†
from vllm import LLM, SamplingParams
import torch
from dataset_cons import DatasetRetriever

import random
import numpy as np
import torch

from datasets import load_dataset, Dataset
from copy import deepcopy

seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)


class LLM_Reasoning_Graph_Baseline:
    def __init__(self, args):
        self.args = args
        self.data_path = args.data_path
        self.dataset_name = args.dataset_name
        self.split = args.split
        self.model_name = args.model_name
        self.save_path = args.save_path
        self.demonstration_path = args.demonstration_path
        self.mode = args.mode   # Direct /CoT /RAG
        
        self.all_data_switch = args.all_data_switch  # æ˜¯å¦å¯¹å®Œæ•´æ•°æ®é›†è¿›è¡Œæµ‹è¯•
        self.batch_test = args.batch_test  # æ˜¯å¦è¿›è¡Œbatchæµ‹è¯•
        self.batch_size = args.batch_size  # batch sizeå¤§å°
        self.vllm_switch = args.use_vllm  # æ˜¯å¦ä½¿ç”¨vllmè¿›è¡ŒåŠ é€Ÿ
        self.max_new_tokens = args.max_new_tokens
        self.zero_shot = args.zero_shot   # åœ¨éragçš„æ¨¡å¼ä¸‹ç”Ÿæ•ˆ
        self.rag_result_path = args.rag_result_path
        self.system_prompt_dir = args.system_prompt_dir
        self.user_template_dir = args.user_template_dir
        self.reverse_rag_order = args.reverse_rag_order
        self.rerank = args.rerank
        self.dtype = args.dtype
        # å¯¹éœ€è¦çš„éƒ¨åˆ†æ•°æ®è¿›è¡Œåˆå§‹åŒ–
        self.para_init()

        self.tokenizer, self.model= self.load_model()
        if not self.vllm_switch:
            self.device = self.model.device

        self.label_phrase = 'The correct option is:'

    # 2025.11.11 separate some init code from init
    def para_init(self):
        # æ¨¡å‹è·¯å¾„åˆå§‹åŒ–
        if self.model_name == "qwen7":
            self.model_path = "../llms/Qwen2.5-7B-Instruct"
        elif self.model_name == "qwen14":
            self.model_path = "../llms/Qwen2.5-14B-Instruct"
        elif self.model_name == "qwen3-8":
            self.model_path = "../llms/Qwen3-8B"
        elif self.model_name == "qwen3-14":
            self.model_path = "../llms/Qwen3-14B"
        elif self.model_name == "qwen3-32":
            self.model_path = "../llms/Qwen3-32B"
        elif self.model_name == "llama3-8":
            self.model_path = "../llms/llama3.1-8B-Instruct"
        else:
            self.model_path = "../llms/"
        
        # é’ˆå¯¹ä¸åŒmodeçš„å‚æ•°åˆå§‹åŒ–
        if self.mode == "RAG":   # è¯´æ˜å½“å‰æ˜¯ragæ¨¡å¼ï¼Œéœ€è¦åŠ è½½æ£€ç´¢åº“
            # RAGæ£€ç´¢å™¨åŠ è½½éƒ¨åˆ†
            self.rag_topk = args.top_k   # æ£€ç´¢çš„æ ·ä¾‹ä¸ªæ•°
            self.rag_icl_num = args.icl_num   # ç”¨äºä¸Šä¸‹æ–‡å­¦ä¹ çš„å±•ç¤ºæ ·ä¾‹ä¸ªæ•°  
            self.db_name = args.db_name 
            self.index_path = args.index_path
            self.dataset_retriever = DatasetRetriever(self.args)
            self.db_type = args.db_type
            # ragæ‰€ç”¨çš„icl templateæ–‡ä»¶è·¯å¾„ï¼Œç”¨äºåŒ…è£…æ£€ç´¢åˆ°çš„document
            self.icl_template_file =  f"{'gsm8k' if self.db_name == 'gsm8k' else 'LogicalReasoning'}_ICL_template.txt"
            self.icl_template_path = os.path.join(self.user_template_dir, self.icl_template_file)
        # å°†zero-shotçš„é€»è¾‘ä¹ŸåŠ åˆ°è¿™é‡Œ
        elif self.zero_shot:
            self.testing_type = "0-shot"
        else:
            self.testing_type = "few-shot"
        
        # role prompt è·¯å¾„åˆå§‹åŒ–
        if self.dataset_name == "gsm8k":
            self.prompt_file = f"{self.dataset_name}_{self.mode}{'_0shot' if self.zero_shot else ''}.txt"
        else:
            self.prompt_file = f"LogicalReasoning_{self.mode}{'_0shot' if self.zero_shot else ''}.txt"
        self.system_prompt_path = os.path.join(self.system_prompt_dir, self.prompt_file)
        print(f"system prompt file path: {self.system_prompt_path}")
        
        # user promptè·¯å¾„åˆå§‹åŒ–
        self.user_prompt_path = os.path.join(self.user_template_dir, self.prompt_file)
        print(f"user prompt file path: {self.user_prompt_path}")
        
        # å¾…æ£€æŸ¥åˆ¤æ–­é€»è¾‘æ˜¯å¦æ­£ç¡®åŠå®Œå–„ï¼Œ prompt creatoråˆå§‹åŒ–
        if self.mode == "RAG":
            if self.rag_icl_num > 0:
#                 self.prompt_creator = self.rag_prompt_creator
                self.prompt_creator = self.prompt_LSAT
            else:
                self.prompt_creator = self.prompt_LSAT
        else:
            self.prompt_creator = self.prompt_LSAT

        # ç»“æœå­˜å‚¨è·¯å¾„åˆå§‹åŒ–
        # ç»Ÿä¸€å®šä¹‰å­˜å‚¨è·¯å¾„
        if self.mode == "RAG":
            self.save_file = os.path.join(self.save_path, f'{self.mode}{self.rag_icl_num}_{self.db_name}_{self.db_type}{"_reversed" if self.reverse_rag_order else ""}_{self.dataset_name}_{self.split}_{self.model_name}.json')
            # laskaå®šä¹‰ä¸€ä¸ªä¿å­˜æ£€ç´¢ä¸­é—´ç»“æœçš„æ–‡ä»¶
            if not os.path.exists(self.rag_result_path):
                os.makedirs(self.rag_result_path)
            self.retrieval_save_file = os.path.join(self.rag_result_path, f'retrieval_{self.db_name}_{self.db_type}_{self.dataset_name}_{self.split}.json')   # åªä¸æ–‡ä»¶æœ‰å…³
            self.retrieval_writer = open(self.retrieval_save_file, 'w') 
        else:
            self.save_file = os.path.join(self.save_path, f'{self.mode}_{self.testing_type}_{self.dataset_name}_{self.split}_{self.model_name}.json')
        
        # æ‰“å°éƒ¨åˆ†å‚æ•°
        print("="*16+"parameteres"+"="*16)
    
        self.print_self()
        print("="*16+"parameteres"+"="*16)
    # æ‰“å°å‚æ•°
    def print_self(self):
        for k,v in self.__dict__.items():
            print(f"{k}:{v}")

    # laska system promptåŠ è½½å‡½æ•°
    def load_system_prompt(self):
        with open(self.system_prompt_path, 'r') as f:
            system_prompt = f.read()
        return system_prompt

    # 2025.11.11 åŠ è½½user promptçš„templateéƒ¨åˆ†ï¼Œç”¨äºæ„å»ºæ•°æ®
    def load_user_prompt_template(self):
        with open(self.user_prompt_path, "r") as f:
            user_prompt = f.read()
        return user_prompt
    
    # 2025.11.11 å¢åŠ icl_promptéƒ¨åˆ†
    def load_icl_template(self):
        with open(self.icl_template_path, "r") as f:
            icl_template = f.read()
        return icl_template
    
    # laska æ¨¡å‹åŠ è½½éƒ¨åˆ†     
    def load_model(self):
        # vllm æ–°å¢
        if self.vllm_switch:
            print("ä½¿ç”¨vllmè¿›è¡Œæ¨¡å‹åŠ è½½å’Œæ¨ç†")
            print("loading model from:", self.model_path)
            model = LLM(model=self.model_path, tokenizer=self.model_path,tensor_parallel_size=torch.cuda.device_count(), max_model_len=32768,dtype=self.dtype, trust_remote_code=True, gpu_memory_utilization=0.9)
            tokenizer = AutoTokenizer.from_pretrained(self.model_path, padding_side='left')
#             self.sampling_params = SamplingParams(temperature=0, max_tokens=self.max_new_tokens, top_p=0.95, top_k=40, n=1)
            self.sampling_params = SamplingParams(temperature=0, max_tokens=self.max_new_tokens, top_p=1, top_k=1, n=1)
            return tokenizer, model
        else:
            print("ç›´æ¥åŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç†")
            tokenizer = AutoTokenizer.from_pretrained(self.model_path, padding_side='left')   # ç›´æ¥ä»æœ¬åœ°è·¯å¾„è¿›è¡ŒåŠ è½½
            print("loading model from:", self.model_path)
            model = AutoModelForCausalLM.from_pretrained(self.model_path, dtype="auto", device_map="auto")
            print("loading complete")
            return tokenizer, model

    # 20251216 æ–°å¢ä¸€ä¸ªconeåˆ©ç”¨æ¡ä»¶æ¦‚ç‡é‡æ’çš„åŠŸèƒ½
    def cone_rerank(self, retrieved_results, test_example):
        icl_template = self.load_icl_template()   # éœ€è¦åŠ è½½æ¨¡æ¿ï¼Œæ‹¼æ¥ä¹‹åç»™æ¨¡å‹ï¼ŒæŸ¥çœ‹å¯¹lossæ˜¯å¦æœ‰æå‡
        user_prompt_template = self.load_user_prompt_template()
        role_content = self.load_system_prompt()
        chat_template_texts = []
        mask_lengths_idxs = []
        max_length = 0
        for result in retrieved_results:
            # å°†å½“å‰çš„æ£€ç´¢ç»“æœæ‹¼æ¥æˆ
            new_icl_template = deepcopy(icl_template)
            cur_icl = new_icl_template.format(
                context=result['context'],
                question=result['question'],
                options='\n'.join([opt.strip() for opt in result.get("options", [])]),
                cot=result['cot'],
                answer=result['answer']
                )
            new_user_prompt_template = deepcopy(user_prompt_template)
            cur_full_prompt = new_user_prompt_template.replace("[[DEMONSTRATIONS]]", cur_icl)

            # å…ˆå°†queryç­‰å†…å®¹ä¹Ÿè¿›è¡Œæ›¿æ¢Â 
            if self.dataset_name == "gsm8k":
                question = test_example["question"].strip()
                cur_full_prompt = cur_full_prompt.replace('[[QUESTION]]', question)
            else:
                context = test_example['context'].strip()
                question = test_example['question'].strip()
                options = '\n'.join([opt.strip() for opt in test_example['options']])
                cur_full_prompt = cur_full_prompt.replace('[[CONTEXT]]', context)
                cur_full_prompt = cur_full_prompt.replace('[[QUESTION]]', question)
                cur_full_prompt = cur_full_prompt.replace('[[OPTIONS]]', options)
            # å…ˆæ›¿æ¢æˆä¸ºmessageså½¢å¼ï¼Œå¹¶apply_chat_templateï¼Œç„¶åè®¡ç®—mask_length
            cur_messages = [{"role": "system", "content": role_content},
                            {"role":"user", "content": cur_full_prompt}]
            cur_text = self.tokenizer.apply_chat_template(cur_messages, add_generation_prompt=True, tokenize=False)
            if len(cur_text) > max_length:
                max_length = len(cur_text)
            # å­˜å…¥åˆ—è¡¨ä¾›åç»­æ“ä½œ
            chat_template_texts.append(cur_text)
            
            # è·å–icl demonstrationçš„ç»“æŸä½ç½®ï¼Œè®¡ç®—mask length
            first_context_idx = cur_text.find("Context:")
            second_context_idx = cur_text.find("Context:", first_context_idx+1)
        
            mask_lengths_idxs.append(second_context_idx)
        
        # éœ€è¦ä¸€æ¡æ¡å¯¹textè¿›è¡Œå¤„ç†
        model_input_ids = [self.tokenizer(chat_template_text, return_tensors="pt").input_ids for chat_template_text in chat_template_texts]
        input_mask_lengths = [self.tokenizer(chat_template_texts[idx][:mask_lengths_idxs[idx]], return_tensors="pt").input_ids.shape[1] for idx in range(len(mask_lengths_idxs))]
        # éœ€è¦æ³¨æ„ã€‚åˆ—è¡¨é‡Œé¢çš„inputæ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œ1*len

        # vllm cone params
        vllm_cone_params = SamplingParams(temperature=0, max_tokens=1, prompt_logprobs=20, detokenize=False)
        # è°ƒç”¨vllmæ¨¡å‹è¿›è¡Œlogitsçš„è·å–
        if self.vllm_switch == True:
            # éœ€è¦æ³¨æ„ï¼Œvllmçš„è¾“å…¥æ˜¯tokenizerä¹‹å‰çš„texts
            vllm_outputs = self.model.generate(chat_template_texts, sampling_params=vllm_cone_params)
            ce_loss = self.ce_loss_cal(model_input_ids, vllm_outputs, input_mask_lengths)
            sorted_idx = torch.argsort(ce_loss)   # é»˜è®¤å‡åº
            # æŒ‰ç…§è·å–åˆ°çš„ce_lossï¼Œå¯¹æ£€ç´¢åˆ°çš„å†…å®¹è¿›è¡Œæ’åº
            return sorted_idx   # è¿”å›idx,ç”¨äºå¯¹æ£€ç´¢ç»“æœè¿›è¡Œå¤„ç†
        else:   # é’ˆå¯¹transformersæ¨¡å‹çš„å¤„ç†æ–¹æ³•
            pass

    # å®šä¹‰ä¸€ä¸ªce_lossçš„è®¡ç®—å‡½æ•°ï¼Œè¾“å…¥æ˜¯æ¨¡å‹inputså’Œoutputs
    def ce_loss_cal(self, input_ids, outputs, mask_lengths):
        all_losses = []
        for ids, out in zip(input_ids, outputs):   # è¿™é‡Œåº”è¯¥å’Œbatch sizeä¸€è‡´ï¼Ÿ
            # è¿™é‡Œçš„idsæ˜¯æ˜¯ä¸€ä¸ª[1,seq_len]çš„äºŒç»´æ•°ç»„
            ids = ids.view(ids.shape[1])   # éœ€è¦å°†äºŒç»´æ•°ç»„idså±•å¼€ä¸ºä¸€ç»´æ•°ç»„ï¼Œé•¿åº¦ç›´æ¥æ˜¯seq_len
            token_losses = []
            for tok_id, lp in zip(ids, out.prompt_logprobs):
        
                if lp is None:
                    token_losses.append(0.0)
                    continue
                info = lp.get(int(tok_id), None)
                if info is None:   
                    token_losses.append(20.0)    # fallback,èµ‹äºˆä¸€ä¸ªå¤§loss
                else:
                    token_losses.append(-info.logprob)
            all_losses.append(torch.tensor(token_losses))
        # å¯¹lossè¿›è¡Œè®¡ç®—
        loss = torch.nn.utils.rnn.pad_sequence(all_losses, batch_first=True, padding_value=0.0)
        
        # æŒ‰ç…§maskçš„é•¿åº¦è¿›è¡Œmask
        mask = torch.zeros_like(loss)
        for i in range(len(mask_lengths)):
            # print(mask_lengths[i], mask[i].shape)
            mask[i, mask_lengths[i]:input_ids[i].shape[1]] = 1
  
        # å°†contextéƒ¨åˆ†çš„lossè¿›è¡Œmask
        loss = mask * loss
        # print(loss)
        ce_loss = torch.sum(loss, 1)
        return ce_loss

    # laska æ„å»ºä½¿ç”¨ragåŠ¨æ€å˜åŒ–demonstrationçš„promptç”Ÿæˆå™¨
    def rag_prompt_creator(self, in_context_example, test_example):
        # 2025.11.11 add system prompt
        role_content = self.load_system_prompt()   # ä¸è®ºæ˜¯ragè¿˜æ˜¯cotçš„system promptéƒ½æ˜¯ä¸€æ ·çš„
        user_prompt_template = self.load_user_prompt_template()  # ç›®å‰è¿™ä¸€éƒ¨åˆ†çš„é€‰æ‹©æ˜¯ä¸ä¸€æ ·çš„
        # é¦–å…ˆè¿›è¡Œæ£€ç´¢ï¼Œå¾—åˆ°ç›¸å…³çš„demonstration
        # æ‰€æœ‰æ•°æ®é›†éƒ½æœ‰questionåŸŸ
        rag_query =test_example["question"].strip()
        retrieved_results = self.dataset_retriever.retrieve(rag_query, self.rag_topk)
        # åˆ¶å®šä¸€ä¸ªtemplate 
        icl_template = self.load_icl_template()
        print(icl_template)
#         icl_template = "Context:\n{context}\nQuestion:\n{question}\nOptions:\n{options}\nReasoning:\n{cot}\nAnswer:\n{answer}\n"
        
        # æ„å»ºæ£€ç´¢çš„æ•°æ®é›†
        overall_demonstration = ""
        for result in retrieved_results[:self.rag_icl_num]:
            overall_demonstration += icl_template.format(
                context=result['context'],
                question=result['question'],
                options='\n'.join([opt.strip() for opt in result.get("options", [])]),
                cot=result['cot'],
                answer=result['answer']
            ) + "\n"
        
        full_in_context_example = user_prompt_template.replace("[[DEMONSTRATIONS]]",)
#         full_in_context_example = head_template + "\n" + overall_demonstration
        # å°†éœ€è¦æµ‹è¯•çš„å†…å®¹è¿›è¡Œæ‹¼æ¥
        test_template = "Context:\n{context}\nQuestion:\n{question}\nOptions:\n{options}\nReasoning:"
#         print(test_example)
#         print(test_example["context"])
        test_example_str = test_template.format(context=test_example['context'],
                                                question=test_example['question'],
                                                options='\n'.join([opt.strip() for opt in test_example['options']]))
        # æ‹¼æ¥æˆä¸ºæœ€ç»ˆç»™æ¨¡å‹è¿›è¡Œæµ‹è¯•çš„æ ·ä¾‹
        full_prompt = full_in_context_example + "\n" + test_example_str
        role_content = "You are a logical task solver. Follow the demonstrationa to solve the new question. Remember to think step by step with concise chain-of-thought, and adhere to the context related to the question. Then on a new line, output exactly: 'The correct option is: A' or 'The correct option is: B"
        messages = [
            {"role":"system", "content":role_content},
            {"role":"user", "content": full_prompt}
            ]
        print(messages)
        # laska ä¿®æ”¹ï¼Œé’ˆå¯¹æœ¬åœ°æ¨¡å‹ï¼Œè¿”å›messages
        # æ¯æ£€ç´¢ä¸€æ¡ï¼Œå°†æ£€ç´¢ç»“æœå†™å…¥æ–‡ä»¶
        retrieval_record = {
            'context': test_example['context'],
            'question': test_example['question'],
            'retrieved_demonstrations': full_in_context_example
        }
        # å†™å…¥jsonæ–‡ä»¶
        self.retrieval_writer.write(json.dumps(retrieval_record, ensure_ascii=False) + '\n')
        return messages
    
    # é’ˆå¯¹few-shotï¼Œç”Ÿæˆpromptï¼Œè¯¥éƒ¨åˆ†å®Œæˆçš„æ˜¯åœ¨å•ä¸ªæ ·ä¾‹ä¹‹å‰æ·»åŠ few-shotçš„ç¤ºä¾‹
    def prompt_LSAT(self, in_context_example, test_example):
        # 2025.11.11 add system prompt
        role_content = self.load_system_prompt()   # ä¸è®ºæ˜¯ragè¿˜æ˜¯cotçš„system promptéƒ½æ˜¯ä¸€æ ·çš„   
        user_prompt_template = self.load_user_prompt_template()  # ç›®å‰è¿™ä¸€éƒ¨åˆ†çš„é€‰æ‹©æ˜¯ä¸ä¸€æ ·çš„     
        # è¿™ä¸€éƒ¨åˆ†åˆ†æ”¯é€»è¾‘å¾…éªŒè¯ä»£ç æ­£ç¡®æ€§
        if self.mode == "RAG":
            full_prompt = user_prompt_template
        elif self.zero_shot == True:
            full_prompt = user_prompt_template
        else:
            full_prompt = in_context_example
        # 20251202 æ·»åŠ æ‰“å°ä¿¡æ¯æ ‡è®°
        if not hasattr(type(self).prompt_LSAT, "_has_run"):
            print("ğŸ‘‰ self.prompt_LAST è¢«é¦–æ¬¡è°ƒç”¨ï¼Œæ‰“å°æç¤ºä¿¡æ¯")
            print("-"*36)
            print("current role_content is :")
            print(role_content)
            print("-"*16)
            print("current user template is:")
            print(user_prompt_template)
            print("-"*16)
            print("full prompt is:")
            print(full_prompt)            
            print("-"*36)
            type(self).prompt_LSAT._has_run = True

        # 2025.11.11 å¢åŠ ragçš„promptæ„é€ 
        # æ‰€æœ‰æ•°æ®é›†éƒ½æœ‰questionåŸŸ
        if self.mode == "RAG":
#             print(test_example)
            rag_query = test_example["question"].strip()
            retrieved_results = self.dataset_retriever.retrieve(rag_query, self.rag_topk)   # æ£€ç´¢å›æ¥çš„ä¼šæ¯”å®é™…éœ€è¦çš„å¤š
            # åˆ¶å®šä¸€ä¸ªtemplate 
            icl_template = self.load_icl_template()
#             print(icl_template)
            # æ„å»ºæ£€ç´¢çš„æ•°æ®é›†
            overall_demonstration = ""
            
            # laska 20251216æ–°å¢rerank é€»è¾‘
            if self.rerank:
                # print(retrieved_results[0])
                sorted_idx = self.cone_rerank(retrieved_results, test_example)
                # å¯¹retrieved_resultsè¿›è¡Œé‡æ’åº
                retrieved_results = [retrieved_results[idx] for idx in sorted_idx]
                # print("after sorted~")
                # print(retrieved_results[0])
                # exit()

            # å…ˆæ ¹æ®éœ€è¦å€’åº
            if self.reverse_rag_order:
                candidates = retrieved_results[:self.rag_icl_num][::-1]   # å€’åºæŒ‘å‰ N
            else:
                candidates = retrieved_results[:self.rag_icl_num]         # æ­£åºæŒ‘å‰ N

            # ç”¨ candidates æ¥å¾ªç¯  
            for result in candidates:
                overall_demonstration += icl_template.format(
                    context=result['context'],
                    question=result['question'],
                    options='\n'.join([opt.strip() for opt in result.get("options", [])]),
                    cot=result['cot'],
                    answer=result['answer']
                ) + "\n"
#             print("before replace:\n", overall_demonstration)
            full_prompt = user_prompt_template.replace("[[DEMONSTRATIONS]]", overall_demonstration)
        
        # é’ˆå¯¹role palyçš„æ¨¡å‹ï¼Œéœ€è¦åŠ ä¸Šuserç­‰è§’è‰²
        # é’ˆå¯¹gsm8kçš„å¤„ç†é€»è¾‘ä¸ä¸€æ ·
        if self.dataset_name == "gsm8k":
            question = test_example['question'].strip()
            full_prompt = full_prompt.replace('[[QUESTION]]', question)
        else:
            context = test_example['context'].strip()
            question = test_example['question'].strip()
            options = '\n'.join([opt.strip() for opt in test_example['options']])
            full_prompt = full_prompt.replace('[[CONTEXT]]', context)
            full_prompt = full_prompt.replace('[[QUESTION]]', question)
            full_prompt = full_prompt.replace('[[OPTIONS]]', options)
        messages = [
            {"role":"system", "content":role_content},
            {"role":"user", "content": full_prompt}
            ]
        if self.mode == "RAG":
            # laska ä¿®æ”¹ï¼Œé’ˆå¯¹æœ¬åœ°æ¨¡å‹ï¼Œè¿”å›messages
            # æ¯æ£€ç´¢ä¸€æ¡ï¼Œå°†æ£€ç´¢ç»“æœå†™å…¥æ–‡ä»¶
            retrieval_record = {
                'context': test_example.get('context',""),
                'question': test_example['question'],
                'retrieved_demonstrations': full_prompt
            }
            # å†™å…¥jsonæ–‡ä»¶
            self.retrieval_writer.write(json.dumps(retrieval_record, ensure_ascii=False) + '\n')
        return messages

    # é’ˆå¯¹zero-shotï¼Œç›´æ¥ç”Ÿæˆprompt
    def prompt_LSAT_zero_shot(self, in_context_example, test_example):
        # 2025.11.11 add system prompt
        role_content = self.load_system_prompt()
        user_prompt_template = self.load_user_prompt_template()  # ç›®å‰è¿™ä¸€éƒ¨åˆ†çš„é€‰æ‹©æ˜¯ä¸ä¸€æ ·çš„
        full_prompt = user_prompt_template
        # é’ˆå¯¹gsm8kçš„å¤„ç†é€»è¾‘ä¸ä¸€æ ·
        if self.dataset_name == "gsm8k":
            question = test_example['question'].strip()
            # full_prompt = f"Problem: {question}\nReasoning:"
            full_prompt = full_prompt.replace('[[QUESTION]]', question)
        else:  # é’ˆå¯¹å…¶ä»–é€»è¾‘æ¨ç†çš„æ•°æ®é›†
            context = test_example['context'].strip()
            question = test_example['question'].strip()
            options = '\n'.join([opt.strip() for opt in test_example['options']])
            full_prompt = full_prompt.replace('[[CONTEXT]]', context)
            full_prompt = full_prompt.replace('[[QUESTION]]', question)
            full_prompt = full_prompt.replace('[[OPTIONS]]', options)

        messages = [
            {"role":"system", "content":role_content},  
            {"role":"user", "content": full_prompt}
            ]   
        return messages
       
    # é’ˆå¯¹few-shotçš„å¤„ç†ä»£ç 
    def load_in_context_examples(self):
        with open(os.path.join(self.demonstration_path, f'{self.dataset_name}_{self.mode}.txt')) as f:
            in_context_examples = f.read()
        return in_context_examples

    # laska åŠ è½½æœ¬åœ°æ•°æ®é›†ï¼Œ2025.12.09
    # ä½¿ç”¨HuggingFace datasetsåº“åŠ è½½æœ¬åœ°æ•°æ®é›†
    def load_raw_dataset(self, split):
        """
        ä½¿ç”¨ HuggingFace datasets åº“åŠ è½½æœ¬åœ° JSON / JSONL æ•°æ®ã€‚
        çº¦å®šï¼š
        - gsm8k:  æ–‡ä»¶åä¸º {split}.jsonl
        - å…¶ä»–æ•°æ®é›†: æ–‡ä»¶åä¸º {split}.json
        """
        if self.dataset_name == "gsm8k":
            file_name = f"{split}.jsonl"   # åŸæ¥å°±æ˜¯ jsonl
        else:
            file_name = f"{split}.json"

        data_file = os.path.join(self.data_path, self.dataset_name, file_name)

        # ç”¨ datasets è¯»æœ¬åœ° json/jsonl
        # è¿™é‡Œç”¨ data_files={split: path} çš„å½¢å¼ï¼Œæ–¹ä¾¿ä¿ç•™ split åå­—
        ds_dict = load_dataset(
            "json",
            data_files={split: data_file}
        )
        raw_dataset: Dataset = ds_dict[split]

        print(f"[datasets] Loaded {len(raw_dataset)} examples from {data_file}")
        return raw_dataset

    def load_raw_dataset_old(self, split):
        if self.dataset_name == "gsm8k":
            with open(os.path.join(self.data_path, self.dataset_name, f"{self.split}.jsonl"), 'r') as f:
                raw_dataset = [json.loads(line) for line in f]
            return raw_dataset
        with open(os.path.join(self.data_path, self.dataset_name, f'{split}.json')) as f:
            raw_dataset = json.load(f)
        return raw_dataset
    
    # laska è°ƒç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆç»“æœ
    def model_generate(self, messages):
        # print(type(messages), type(messages[0]), len(messages), len(messages[0]))
        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        # applyä¹‹åå¾—åˆ°çš„textæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œè€Œtokenizerçš„è¾“å…¥éœ€è¦æ˜¯ä¸€ä¸ªlistï¼Œæ‰€ä»¥éœ€è¦[text]
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.device)
        generated_ids = self.model.generate(**model_inputs, do_sample=False, max_new_tokens=self.max_new_tokens)
        # model.generateè¿”å›çš„ç»“æœæ˜¯ä¸€ä¸ª[[... ...]]çš„äºŒç»´listï¼Œå•æ¡å’Œbatchçš„åŒºåˆ«åœ¨äºç¬¬ä¸€ç»´çš„é•¿åº¦
        # print("--------the final answer is !!!!---------")
        # print(generated_ids)
        # é’ˆå¯¹å•æ¡æ•°æ®ï¼Œéœ€è¦å»æ‰å‰é¢input_idçš„éƒ¨åˆ†
        generated_ids = generated_ids[:,len(model_inputs.input_ids[0]):]
        # print(generated_ids.shape)
        # print(generated_ids)
        # responseçš„è¿”å›æ˜¯åˆ—è¡¨çš„å½¢å¼ï¼Œé’ˆå¯¹å•æ¡æ•°æ®çš„æµ‹è¯•ï¼Œéœ€è¦å–ç¬¬1æ¡å…ƒç´ 
        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        # print("++++++the response is ++++++++++")
        # print(response)
        return response
    
    # laska å®šä¹‰ä¸€ä¸ªé’ˆå¯¹batchæ•°æ®è¿›è¡Œè§£ç çš„å‡½æ•°
    def model_generate_batch(self, messages_list):
        texts = [self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) for messages in messages_list]
        # texts ç»è¿‡applyå‡½æ•°ä¹‹åæ˜¯strçš„åˆ—è¡¨
        # print(type(texts), len(texts), type(texts[0]), len(texts[0]))
        # åˆ†ä¸ºvllmå’Œæ™®é€šè°ƒç”¨ä¸¤éƒ¨åˆ†
        if self.vllm_switch:
            # vllmçš„è°ƒç”¨ï¼Œä¸model generateä¸åŒï¼Œä¸éœ€è¦è¿›è¡Œtokenizerçš„encode
            outputs = self.model.generate(texts, sampling_params=self.sampling_params)
            # print(outputs)
            responses = [output.outputs[0].text for output in outputs]
            # print(responses)
                
        else:
            model_inputs = self.tokenizer(texts, return_tensors="pt", padding=True).to(self.device)
            generated_ids = self.model.generate(**model_inputs, do_sample=False, max_new_tokens=self.max_new_tokens)
            # print("--------the final answer is !!!!---------")
            # print(generated_ids)
            generated_ids = [output_ids[len(input_ids):] for output_ids, input_ids in zip(generated_ids, model_inputs.input_ids)]
            # response
            responses = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
            # print(responses)
        return responses  # è¿”å›å½“å‰ä¸€ä¸ªbatchçš„ç»“æœ
    
    # laska å®šä¹‰ä¸€ä¸ªè°ƒç”¨å…¥å£, åˆ†é…æ˜¯batchè¿˜æ˜¯å•æ¡
    def generation_entrance(self):
        if self.batch_test:
            print("è¿›è¡Œbatchæµ‹è¯•")
            self.batch_reasoning_graph_generation(batch_size=self.batch_size)
        else:
            print("è¿›è¡Œå•æ¡æµ‹è¯•")
            self.reasoning_graph_generation()

    def reasoning_graph_generation(self):
        # load raw dataset
        raw_dataset = self.load_raw_dataset(self.split)
        print(f"Loaded {len(raw_dataset)} examples from split = {self.split}.")

        # load in-context examples
        in_context_examples = self.load_in_context_examples()
        
        outputs = []
        for example in tqdm(raw_dataset):
            question = example['question']

            # create prompt
            full_prompt = self.prompt_creator(in_context_examples, example)
#             print(full_prompt)
            # ä¿®æ”¹è¿™éƒ¨åˆ†æ¨¡å‹ç”Ÿæˆä»£ç 
#             output = self.openai_api.generate(full_prompt)
            # output = self.model_generate(full_prompt)
            # laska ï¼Œä¿®æ”¹ä¸ºåŒä¸€ä¸ªå‡½æ•°è°ƒç”¨ï¼Œå”¯ä¸€çš„å·®åˆ«æ˜¯listä¸­çš„å…ƒç´ ä¸ªæ•°
            # æ­¤å¤„çš„full_promptæ˜¯ä¸€ä¸ªlistï¼Œqwençš„è¾“å…¥æ ¼å¼ï¼ŒåŒ…å«systemå’Œuserä¸¤ä¸ªéƒ¨åˆ†
            outputs = self.model_generate_batch(full_prompt)
            output = outputs[0]  # å–å‡ºå•æ¡æ•°æ®çš„ç»“æœ
            # get the answer
            label_phrase = self.label_phrase    #  self.label_phrase = 'The correct option is:'
            generated_answer = output.split(label_phrase)[-1].strip()
            generated_reasoning = output.split(label_phrase)[0].strip()

            # create output
            output = {'id': example['id'], 
                      'question': question, 
                      'answer': example['answer'], 
                      'predicted_reasoning': generated_reasoning,
                      'predicted_answer': generated_answer}
            outputs.append(output)
            # å®šä¹‰ä¸€ä¸ªæµ‹è¯•çš„å¼€å…³
            if self.all_data_switch == False:
                print(full_prompt)
                print("å½“å‰åªæµ‹è¯•ä¸€æ¡æ•°æ®ï¼ŒæŸ¥çœ‹ç»“æœå³å¯")
                print(output)
                break
        # save outputs        
        with open(self.save_file, 'w') as f:
            json.dump(outputs, f, indent=2, ensure_ascii=False)

    # laska å®šä¹‰ä¸€ä¸ªbatchæµ‹è¯•çš„ä»£ç 
    def batch_reasoning_graph_generation(self, batch_size=10):
        # load raw dataset
        raw_dataset = self.load_raw_dataset(self.split)
        print(f"Loaded {len(raw_dataset)} examples from {self.split} split.")

        # load in-context examples,é’ˆå¯¹é0-shotçš„åœºæ™¯
        if self.mode in ["CoT", "Direct"] and not self.zero_shot:    # ragå½¢å¼éœ€è¦è‡ªè¡ŒæŸ¥æ‰¾context
            in_context_examples = self.load_in_context_examples()
        else:   # rag/cot-0shot
            in_context_examples = ""
            
        outputs = []
        # split dataset into chunks
        num_examples = len(raw_dataset)
#         dataset_chunks = [raw_dataset[i:i + batch_size] for i in range(0, len(raw_dataset), batch_size)]
        # for chunk in tqdm(dataset_chunks):
        for start in tqdm(range(0, num_examples, batch_size)):
            end = min(start + batch_size, num_examples)
            chunk = raw_dataset.select(range(start, end))
            # create prompt
            full_prompts = [self.prompt_creator(in_context_examples, example) for example in chunk]
            # è°ƒç”¨æ¨¡å‹è¿›è¡Œbatchçš„é¢„æµ‹
            batch_output = self.model_generate_batch(full_prompts)
            for sample, output in zip(chunk, batch_output):
                # get the answer
                dict_output = self.update_answer(sample, output)
                outputs.append(dict_output)
            # å®šä¹‰ä¸€ä¸ªæµ‹è¯•çš„å¼€å…³
            if self.all_data_switch == False:
                print(full_prompts)
                print("å½“å‰åªæµ‹è¯•ä¸€ä¸ªbatchæ•°æ®ï¼ŒæŸ¥çœ‹ç»“æœå³å¯")
                print(outputs)
                break
        # save outputs        
        with open(self.save_file, 'w') as f:
            json.dump(outputs, f, indent=2, ensure_ascii=False)

    def update_answer(self, sample, output):
        # é’ˆå¯¹gsm8kæ˜¯å•ç‹¬çš„å¤„ç†
        if self.dataset_name == "gsm8k":
            label_phrase = "Final answer:"
            generated_answer = output.split(label_phrase)[-1].strip().lstrip("<").rstrip(">")
            generated_reasoning = output.split(label_phrase)[0].strip()
        # é’ˆå¯¹å…¶ä»–é€»è¾‘æ¨ç†çš„æ•°æ®é›†ProntoQAã€ProofWriterç­‰
        else:    
            if self.mode in ["Direct", "CoT", "RAG"]:
                label_phrase = self.label_phrase
            elif self.mode in ["Logical"]:
                label_phrase = "Answer:"
                
            if label_phrase not in output and label_phrase.lower() in output:
                label_phrase = label_phrase.lower()
            generated_answer = output.split(label_phrase)[-1].strip()
            if generated_answer.lower() == "true":
                generated_answer = "A"
            elif generated_answer.lower() == "false":
                generated_answer = "B"
            generated_reasoning = output.split(label_phrase)[0].strip()
        dict_output = {'id': sample['id'], 
                        'question': sample['question'], 
                        'answer': sample['answer'], 
                        'predicted_reasoning': generated_reasoning,
                        'predicted_answer': generated_answer,
                        'generation_context':output}
        return dict_output

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_path', type=str, default='../data')
    parser.add_argument('--dataset_name', type=str)
    parser.add_argument('--split', type=str)
    parser.add_argument('--save_path', type=str, default='./results')
    parser.add_argument('--demonstration_path', type=str, default='./icl_examples')
#     parser.add_argument('--model_path', type=str, default='../llms')
    parser.add_argument('--model_name', type=str)
    parser.add_argument('--stop_words', type=str, default='------')
    parser.add_argument('--mode', type=str, help='Direct or CoT or logical', default='Direct')
    parser.add_argument('--max_new_tokens', type=int)
    # laskaå®šä¹‰ä¸€ä¸ªé’ˆå¯¹0-shotçš„ä»£ç 
    parser.add_argument('--zero_shot', default=False, action='store_true')
    # laska å®šä¹‰ä¸€ä¸ªbatchæµ‹è¯•çš„å¼€å…³
    parser.add_argument('--batch_test', default=False, action='store_true')
    parser.add_argument('--batch_size', type=int, default=8)
    # å®šä¹‰ä¸€ä¸ªvllmçš„å¼€å…³
    parser.add_argument('--use_vllm', default=False, action='store_true')
    # laska å®šä¹‰ä¸€ä¸ªé’ˆå¯¹æ˜¯å¦å¯¹å®Œæ•´æ•°æ®é›†è¿›è¡Œæµ‹è¯•çš„å¼€å…³
    parser.add_argument('--all_data_switch', help='å½“å‰æ˜¯å¦éœ€è¦å¯¹æ‰€æœ‰æ•°æ®é›†è¿›è¡Œæµ‹è¯•(True)ï¼Œè¿˜æ˜¯æµ‹è¯•ä»£ç åŠŸèƒ½(Fasle:åªæµ‹è¯•ä¸€æ¡æ•°æ®å°±å¯ä»¥)', default=False, action='store_true')
    # 10.27 å°†system promptæ”¾åœ¨æ–‡ä»¶ä¸­è¿›è¡ŒåŠ è½½
    parser.add_argument('--system_prompt_dir', type=str, default='./system_prompt', help="å®šä¹‰å­˜æ”¾system promptçš„æ–‡ä»¶è·¯å¾„")
    # parser.add_argument('--prompt_file', help="å®šä¹‰system promptçš„æ–‡ä»¶è·¯å¾„", type=str, default='logical_prompt_1.txt')
    # 11.7 å°†ragåŠŸèƒ½ç›´æ¥åŠ è¿›æ¥
    parser.add_argument('--db_name', type=str, default='gsm8k', help="æ‰€ä½¿ç”¨çš„RAG dbçš„åå­—")  # ç”¨äºæ£€ç´¢çš„æ•°æ®åº“åç§°
    parser.add_argument('--index_path', type=str, default='../rag_db', help="RAGå‘é‡æ•°æ®åº“çš„è·¯å¾„")  # RAGå‘é‡æ•°æ®åº“çš„è·¯å¾„
    parser.add_argument('--icl_num', type=int, default=0, help="RAGæ£€ç´¢åä½¿ç”¨çš„ç¤ºä¾‹ä¸ªæ•°")  # RAGæ£€ç´¢åä½¿ç”¨çš„ç¤ºä¾‹ä¸ªæ•°
    parser.add_argument('--top_k', type=int, default=3, help="RAGæ£€ç´¢çš„top kä¸ªæ•°")  # RAGæ£€ç´¢çš„top kä¸ªæ•°
    parser.add_argument('--rag_result_path', type=str, default='./rag_results', help="RAGæ£€ç´¢ä¸­é—´ç»“æœçš„ä¿å­˜è·¯å¾„")  # RAGæ£€ç´¢ä¸­é—´ç»“æœçš„ä¿å­˜è·¯å¾„
    parser.add_argument("--db_type", type=str, help="å¯é€‰çš„langchain dbç±»å‹ï¼Œembeddingæˆ–è€…bm25", default="embedding")
    # 2025.11.11 user_template_dir
    parser.add_argument("--user_template_dir", type=str, default="./user_template", help="ç”¨äºå­˜æ”¾user templateæ–‡ä»¶çš„dirè·¯å¾„")
    parser.add_argument("--dtype", type=str, default="float16")
    parser.add_argument('--reverse_rag_order', default=False, action='store_true')
    parser.add_argument("--embedding_model", type=str, help="æ‰€ä½¿ç”¨çš„embeddingæ¨¡å‹åå­—", default="../llm/bge-large-en-v1.5")
    # 20251216 æ–°å¢cone çš„rerankåŠŸèƒ½
    parser.add_argument("--rerank", default=False, help="æ˜¯å¦å¯¹æ£€ç´¢çš„å€™é€‰è¿›è¡Œconeé‡æ’åº",action='store_true')
    args = parser.parse_args()
    return args

if __name__ == '__main__':
    args = parse_args()
    llm_problem_reduction = LLM_Reasoning_Graph_Baseline(args)
    # å°è¯•å…¨éƒ¨ç›´æ¥è°ƒç”¨batchçš„ç”Ÿæˆä»£ç 
    llm_problem_reduction.generation_entrance()
#     llm_problem_reduction.batch_reasoning_graph_generation(batch_size=10)
    # llm_problem_reduction.reasoning_graph_generation()
